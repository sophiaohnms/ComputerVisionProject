GENERAL TIPS AND TASKS TO DO AFTER Q&A SESSION (16/4):
(Mostly is how Hakon has achieved baseline 2) 

- ImageNet dataset used fo pretrained models is not that important, since the transfer from waymo to TDT. (This is something Sophia wondered the other day)

- No need to freeze layers when transfering from waymo to TDT.

- Using ResNet* as backbone for the first feature map [30, 40] and then adding residual blocks to get the rest of the resolutions. The basic residual block is nothing more than convolutions and downsampling: https://github.com/pytorch/vision/blob/7b60f4db9707d7afdbb87fd4e8ef6906ca014720/torchvision/models/resnet.py#L35

(This is something we kinda already did on ResNet34. We used handmade layers instead, but that should be fine. Now we can try it on ResNet50)

- SampleCrop and RandomMirror, only augmentation on Waymo.

- Optimizer SGD, as it came with the original code from assignment 4. 

- IMPORTANT! Check prior_idx = -1 (last feature) in the visualize_priors notebook. Might be that the prior boxes 4:3 are not correctly implemented. The impact in performance should be noticeable. (CÃ©sar will take care of this)


- *Inception v3 might be a good model to try (Resnet + GoogleNet), but ResNet should be enough to beat baseline2. 

How to transfer from waymo to tdt?
- Change config file /tdt4265.yaml (or whatever) when training and change the output path and delete the logs. 

______________________________________________________________________________________


TO DO:
Data augmentation -----> Transform "Expand()" breaks the model? (Total Loss was around 800-1000)

Optimizers: ADAM or SGD(with and without nesterov)

Learning rate: scheduler 
Freeze layers on the pre-trained model for Waymo dataset?



SOLVED:
Feature maps: [2,2] or [3,3]* (Adjust the prior_box strides) ---> [3,3]
Out_channels: why [128, 256, 512, 128, 64, 64] and no others? ---> Better to keep increasing channels. 
Transform: Normalize() ---> SubstractMeans + /std
